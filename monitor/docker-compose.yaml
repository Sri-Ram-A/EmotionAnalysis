version: "3.9"

services:
  # --- ML MODEL SERVING LAYER ---
  tensorflow-serving:
    image: tensorflow/serving:latest-gpu # The blueprint for the container
    container_name: tfx          # The custom name for this container
    restart: unless-stopped              # Keeps it running even if it crashes
    runtime: nvidia                      # Tells Docker to use your GPU (--all)
    environment:
      - NVIDIA_VISIBLE_DEVICES=all       # Grants access to all your GPUs
    ports:
      - "8501:8501"                      # Maps [Your Computer Port] : [Container Port]
    volumes:
      - /home/srirama/sr_proj/EmotionAnalysis/src/artifacts/recent/gru:/models/gru                      # Links your model folder to the container
      - /home/srirama/sr_proj/EmotionAnalysis/src/artifacts/recent/model.config:/model.config     # Links your config file to the container
    command: >                           # Startup instructions for the model
      --model_name=gru
      --model_base_path=/models/gru
      --rest_api_port=8501
      --monitoring_config_file=/model.config

  # --- METRIC COLLECTION LAYER ---
  prometheus:
    image: prom/prometheus
    container_name: prometheus
    restart: unless-stopped
    ports:
      - "9090:9090"
    volumes:
      - ./prometheus/prometheus.yml:/etc/prometheus/prometheus.yml:ro
    depends_on:                          # Ensures TF-Serving starts first
      - tensorflow-serving

  # --- VISUALIZATION LAYER ---
  grafana:
    image: grafana/grafana-enterprise
    container_name: grafana
    restart: unless-stopped
    user: "0"                            # Runs as root to avoid folder permission issues
    ports:
      - "3000:3000"
    volumes:
      - ./data/grafana:/var/lib/grafana
    depends_on:
      - prometheus

networks:
  default:
    name: monitor_default  # Connects all three so they can talk to each other